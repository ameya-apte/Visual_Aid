{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "[INFO] loading encodings...\n",
      "[INFO] starting video stream...\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      ". a man wearing a man in a window  The person is  Ameya. a man is wearing a tie  The person is  Ameya. a woman in a beige shirt and his teddy bear in a tie . a man in a tie talking in front of a window . a man is sitting in a shop holding a white and brown dog in a shop talking on a bar with a hair dryer and a tie  The person is  Ameya. a man in a tie  The person is  Ameya\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "try:\n",
    "    import cv2\n",
    "except:\n",
    "    import sys\n",
    "    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "    import cv2\n",
    "import recognize\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "\n",
    "\n",
    "language = 'en'\n",
    "\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "f = open('train_captions_1.pickle', 'rb') #read pickled captions from training, download from drive if trained on colab\n",
    "train_captions = pickle.load(f)\n",
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 10000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,oov_token=\"<unk>\",filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "max_length = 49\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "    \n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n",
    "    \n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "checkpoint_path = \"checkpoint_2.1\"   #ADD CHECKPOINT FILE DOWNLOADED FROM DRIVE AFTER TRAINING ON COLAB.\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "\n",
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    # temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    # img_tensor_val = image_features_extract_model(image)\n",
    "    # img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(image)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "#         attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n",
    "\n",
    "def preprocess(frame):\n",
    "    img = np.expand_dims(frame, axis=0)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    feature = image_features_extract_model(img)\n",
    "    feature = tf.reshape(feature, (feature.shape[0], -1, feature.shape[3]))\n",
    "    return feature\n",
    "import numpy as np\n",
    "import cv2\n",
    "#import upload\n",
    "#import pyrealsense2 as rs\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "i = 0\n",
    "# import glob\n",
    "while(cap.isOpened()):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    count +=1\n",
    "    # Display the resulting frame\n",
    "    while(count==150):\n",
    "        cv2.imwrite('test'+str(i)+'.jpg', frame)\n",
    "        print(count)\n",
    "        count=0\n",
    "        i += 1\n",
    "        # cv2.imshow(\"frame\",frame)\n",
    "        # count += 1\n",
    "    \n",
    "    cv2.imshow(\"frame\",frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "#for name in glob.glob('/home/akhilesh/Server/frame?.jpg'):\n",
    "#    print(name)\n",
    "#When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "sentence = ''\n",
    "#n=i\n",
    "i=0\n",
    "for i in range(6):\n",
    "    file = 'test{}'.format(i)\n",
    "    image = cv2.imread('/home/ameya/rakathon/'+file+'.jpg')\n",
    "#     image = cv2.imread('/home/ameya/Pictures/garden.jpeg')\n",
    "    name = recognize.recognize(image)\n",
    "    feature = preprocess(image)\n",
    "    result, _ = evaluate(feature)\n",
    "#     print(\"The person is\",name)j99999999\n",
    "    text = ''\n",
    "    p = ''\n",
    "    for i in result:\n",
    "        text += i+' '\n",
    "    for j in name:\n",
    "        p=p+' '+j\n",
    "    if len(name)>0:\n",
    "        text = text[:text.find('<end>')]+' The person is '+p\n",
    "#         print(text)\n",
    "    else:\n",
    "        text = text[:text.find('<end>')]\n",
    "#         print(text)\n",
    "    sentence  = sentence +'.'+' '+text\n",
    "\n",
    "#     print(text,\"The person is\",name)\n",
    " \n",
    "    cv2.imshow('frame', cv2.resize(image, (int(image.shape[1]*0.2), int(image.shape[0]*0.2)), interpolation = cv2.INTER_AREA))\n",
    "    cv2.waitKey(5000)\n",
    "    cv2.destroyAllWindows()\n",
    "print(sentence)\n",
    "myobj = gTTS(text=sentence, lang=language, slow=False)\n",
    "myobj.save(file+'.mp3')\n",
    "#playsound(file+'.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
