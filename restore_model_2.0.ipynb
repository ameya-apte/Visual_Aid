{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# #import magic\n",
    "# import urllib.request\n",
    "# from app import app\n",
    "# from flask import Flask, flash, request, redirect, render_template\n",
    "# from werkzeug.utils import secure_filename\n",
    "# import sys\n",
    "# # app = Flask(__name__)\n",
    "# ALLOWED_EXTENSIONS = set(['txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif','mp4'])\n",
    "\n",
    "# def allowed_file(filename):\n",
    "# \treturn '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\t\n",
    "# @app.route('/')\n",
    "# def upload_form():\n",
    "# \treturn render_template('upload.html')\n",
    "\n",
    "# # run_flag = False\n",
    "\n",
    "# # @app.route('/obstacle')\n",
    "# # def obstacle():\n",
    "# # \tpath = \"/home/akhilesh/Server/frame15.jpg\"\n",
    "# # \tcap = cv2.imread(path)\n",
    "# # \tcv2.imshow(\"frame\",cap)\n",
    "# # \tcv2.waitKey(0)\n",
    "\n",
    "# @app.route('/', methods=['POST'])\n",
    "# def upload_file():\n",
    "# \tif request.method == 'POST':\n",
    "#         # check if the post request has the file part\n",
    "# \t\tif 'file' not in request.files:\n",
    "# \t\t\tflash('No file part')\n",
    "# \t\t\treturn redirect(request.url)\n",
    "# \t\tfile = request.files['file']\n",
    "# \t\tif file.filename == '':\n",
    "# \t\t\tflash('No file selected for uploading')\n",
    "# \t\t\treturn redirect(request.url)\n",
    "# \t\tif file and allowed_file(file.filename):\n",
    "# \t\t\tfilename = secure_filename(file.filename)\n",
    "# \t\t\tfile.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
    "# \t\t\tflash('File successfully uploaded')\n",
    "# \t\t\treturn redirect('/')\n",
    "# \t\telse:\n",
    "# \t\t\tflash('Allowed file types are txt, pdf, png, jpg, jpeg, gif,mp4')\n",
    "# \t\t\treturn redirect(request.url)\n",
    "\n",
    "\t\n",
    "\n",
    "# # @app.route('/obstacle')\n",
    "# # def forward():\n",
    "# # \tglobal run_flag\n",
    "# # \tprint(\"Forward!\")\n",
    "# # \trun_flag = True\n",
    "# # \twhile run_flag :\n",
    "\t\t\n",
    "# \t\t# return 'Alexabot moved forward!'\n",
    "#     #place code here from Obstacle_avoidRobo.py\n",
    "  \t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # @app.route('/')\n",
    "# # def index():\n",
    "# # return 'Hello world'\n",
    "\n",
    "\n",
    "# # @app.route('/stop')\n",
    "# # def stop():\n",
    "# # \tglobal run_flag\n",
    "# # \trun_flag = False\n",
    "\n",
    "\n",
    "\n",
    "# # @app.route('/quit')\n",
    "# # def quit():\n",
    "# # \tfunc = request.environ.get('werkzeug.server.shutdown')\n",
    "# # \tfunc()\n",
    "# # \treturn \"Quitting...\"\n",
    "\n",
    "# # if __name__ == '__main__':\n",
    "\n",
    "\n",
    "# def shutdown_server():\n",
    "#     func = request.environ.get('werkzeug.server.shutdown')\n",
    "#     if func is None:\n",
    "#         raise RuntimeError('Not running with the Werkzeug Server')\n",
    "#     else:\n",
    "#     \tshutdown()\n",
    "\n",
    "# @app.route('/shutdown', methods=['POST'])\n",
    "# def shutdown():\n",
    "#     shutdown_server()\n",
    "#     return 'Server shutting down...'\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # app.run()\n",
    "# \tapp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoVks3OUUiLP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "[INFO] loading encodings...\n",
      "[INFO] starting video stream...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "try:\n",
    "    import cv2\n",
    "except:\n",
    "    import sys\n",
    "    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "    import cv2\n",
    "import recognize\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "\n",
    "\n",
    "language = 'en'\n",
    "\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "f = open('train_captions_1.pickle', 'rb') #read pickled captions from training, download from drive if trained on colab\n",
    "train_captions = pickle.load(f)\n",
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 10000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,oov_token=\"<unk>\",filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "max_length = 49\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "    \n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n",
    "    \n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "checkpoint_path = \"checkpoint_2.1\"   #ADD CHECKPOINT FILE DOWNLOADED FROM DRIVE AFTER TRAINING ON COLAB.\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "\n",
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    # temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    # img_tensor_val = image_features_extract_model(image)\n",
    "    # img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(image)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "#         attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n",
    "\n",
    "def preprocess(frame):\n",
    "    img = np.expand_dims(frame, axis=0)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    feature = image_features_extract_model(img)\n",
    "    feature = tf.reshape(feature, (feature.shape[0], -1, feature.shape[3]))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # import numpy as np\n",
    "# # import cv2\n",
    "# # import upload\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# count = 0\n",
    "# i = 0\n",
    "# # import glob\n",
    "# while(cap.isOpened()):\n",
    "#     # Capture frame-by-frame\n",
    "#     ret, frame = cap.read()\n",
    "#     if ret == False:\n",
    "#         break\n",
    "\n",
    "#     # Our operations on the frame come here\n",
    "#     # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     count +=1\n",
    "#     # Display the resulting frame\n",
    "#     while(count==150):\n",
    "#         cv2.imwrite('test'+str(i)+'.jpg', frame)\n",
    "#         print(count)\n",
    "#         count=0\n",
    "#         i += 1\n",
    "#         # cv2.imshow(\"frame\",frame)\n",
    "#         # count += 1\n",
    "#     int m = i\n",
    "#     cv2.imshow(\"frame\",frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# # for name in glob.glob('/home/akhilesh/Server/frame?.jpg'):\n",
    "#     # print(name)\n",
    "# # When everything done, release the capture\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "a bouquet of leafy vegetables are in a vase with flowers \n",
      "a vase filled with roses inside a vase on a table \n",
      "a vase with a bunch of flowers \n",
      "a vase with a large vase with a large vase with a large vase with a large vase with a large vase with a large vase with a large vase with a large vase with a large vase with a large vase with a large vase with a large\n",
      "a table with a table and chairs \n",
      "a table with a single table and a bar \n",
      "a table with a table with a bar \n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "# import upload\n",
    "\n",
    "cap = cv2.VideoCapture('hello1.mp4')\n",
    "out = cv2.VideoWriter('output.avi', -1, 20.0, (640,480))\n",
    "count = 0\n",
    "i = 0\n",
    "# import glob\n",
    "while(cap.isOpened()):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    out.write(frame)\n",
    "    # Our operations on the frame come here\n",
    "    # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    count +=1\n",
    "    # Display the resulting frame\n",
    "    while(count==150):\n",
    "        cv2.imwrite('test'+str(i)+'.jpg', frame)\n",
    "        print(count)\n",
    "        count=0\n",
    "        i += 1\n",
    "        # cv2.imshow(\"frame\",frame)\n",
    "        # count += 1\n",
    "    m = i\n",
    "    \n",
    "    cv2.imshow(\"frame\",frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# for name in glob.glob('/home/akhilesh/Server/frame?.jpg'):\n",
    "    # print(name)\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "i=0\n",
    "for i in range(m):\n",
    "    file = 'test{}'.format(i)\n",
    "    image = cv2.imread('/home/ameya/rakathon/'+file+'.jpg')\n",
    "#     image = cv2.imread('/home/avitra/rakathon/test_imgs/frame15.jpg')\n",
    "    name = recognize.recognize(image)\n",
    "    feature = preprocess(image)\n",
    "    result, _ = evaluate(feature)\n",
    "#     print(\"The person is\",name)\n",
    "    text = ''\n",
    "    p = ''\n",
    "    for i in result:\n",
    "        text += i+' '\n",
    "    for j in name:\n",
    "        p=p+' '+j\n",
    "    if len(name)>0:\n",
    "        text = text[:text.find('<end>')]+' The person is '+p\n",
    "        print(text)\n",
    "    else:\n",
    "        text = text[:text.find('<end>')]\n",
    "        print(text)\n",
    "#     print(text,\"The person is\",name)\n",
    " \n",
    "    cv2.imshow('frame', cv2.resize(image, (int(image.shape[1]*0.2), int(image.shape[0]*0.2)), interpolation = cv2.INTER_AREA))\n",
    "    cv2.waitKey(5000)\n",
    "    cv2.destroyAllWindows()\n",
    "    myobj = gTTS(text=text, lang=language, slow=False)\n",
    "    myobj.save(file+'.mp3')\n",
    "    #playsound(file+'.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# language = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8jl0GlmUkZe"
   },
   "outputs": [],
   "source": [
    "# image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "#                                                 weights='imagenet')\n",
    "# new_input = image_model.input\n",
    "# hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "# image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s_5qr0CgeZ8L"
   },
   "outputs": [],
   "source": [
    "# f = open('train_captions_1.pickle', 'rb') #read pickled captions from training, download from drive if trained on colab\n",
    "# train_captions = pickle.load(f)\n",
    "# # Choose the top 5000 words from the vocabulary\n",
    "# top_k = 10000\n",
    "# tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,oov_token=\"<unk>\",filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "# tokenizer.fit_on_texts(train_captions)\n",
    "# train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "# tokenizer.word_index['<pad>'] = 0\n",
    "# tokenizer.index_word[0] = '<pad>'\n",
    "# # Create the tokenized vectors\n",
    "# train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jA5nqMStUs1V"
   },
   "outputs": [],
   "source": [
    "# # Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# BUFFER_SIZE = 1000\n",
    "# embedding_dim = 256\n",
    "# units = 512\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# # num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# # Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# # These two variables represent that vector shape\n",
    "# features_shape = 2048\n",
    "# attention_features_shape = 64\n",
    "# max_length = 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVjUpu0TU62x"
   },
   "outputs": [],
   "source": [
    "# class BahdanauAttention(tf.keras.Model):\n",
    "#     def __init__(self, units):\n",
    "#         super(BahdanauAttention, self).__init__()\n",
    "#         self.W1 = tf.keras.layers.Dense(units)\n",
    "#         self.W2 = tf.keras.layers.Dense(units)\n",
    "#         self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "#     def call(self, features, hidden):\n",
    "#     # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "#     # hidden shape == (batch_size, hidden_size)\n",
    "#     # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "#         hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "#     # score shape == (batch_size, 64, hidden_size)\n",
    "#         score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "#     # attention_weights shape == (batch_size, 64, 1)\n",
    "#     # you get 1 at the last axis because you are applying score to self.V\n",
    "#         attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "#     # context_vector shape after sum == (batch_size, hidden_size)\n",
    "#         context_vector = attention_weights * features\n",
    "#         context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "#         return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQ1vb-wOU7p6"
   },
   "outputs": [],
   "source": [
    "# class CNN_Encoder(tf.keras.Model):\n",
    "#     # Since you have already extracted the features and dumped it using pickle\n",
    "#     # This encoder passes those features through a Fully connected layer\n",
    "#     def __init__(self, embedding_dim):\n",
    "#         super(CNN_Encoder, self).__init__()\n",
    "#         # shape after fc == (batch_size, 64, embedding_dim)\n",
    "#         self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.fc(x)\n",
    "#         x = tf.nn.relu(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0JRyLUc3U9vr"
   },
   "outputs": [],
   "source": [
    "# class RNN_Decoder(tf.keras.Model):\n",
    "#     def __init__(self, embedding_dim, units, vocab_size):\n",
    "#         super(RNN_Decoder, self).__init__()\n",
    "#         self.units = units\n",
    "\n",
    "#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "#         self.gru = tf.keras.layers.GRU(self.units,\n",
    "#                                    return_sequences=True,\n",
    "#                                    return_state=True,\n",
    "#                                    recurrent_initializer='glorot_uniform')\n",
    "#         self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "#         self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "#         self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "#     def call(self, x, features, hidden):\n",
    "#     # defining attention as a separate model\n",
    "#         context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "#     # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "#         x = self.embedding(x)\n",
    "\n",
    "#     # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "#         x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "#     # passing the concatenated vector to the GRU\n",
    "#         output, state = self.gru(x)\n",
    "\n",
    "#     # shape == (batch_size, max_length, hidden_size)\n",
    "#         x = self.fc1(output)\n",
    "\n",
    "#     # x shape == (batch_size * max_length, hidden_size)\n",
    "#         x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "#     # output shape == (batch_size * max_length, vocab)\n",
    "#         x = self.fc2(x)\n",
    "\n",
    "#         return x, state, attention_weights\n",
    "    \n",
    "#     def reset_state(self, batch_size):\n",
    "#         return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6QQED6OVAN9"
   },
   "outputs": [],
   "source": [
    "# encoder = CNN_Encoder(embedding_dim)\n",
    "# decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PatgV8uVGCC"
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#     from_logits=True, reduction='none')\n",
    "\n",
    "# def loss_function(real, pred):\n",
    "#     mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "#     loss_ = loss_object(real, pred)\n",
    "\n",
    "#     mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#     loss_ *= mask\n",
    "\n",
    "#     return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LkZYeukdVIrx"
   },
   "outputs": [],
   "source": [
    "# checkpoint_path = \"checkpoint_2.1\"   #ADD CHECKPOINT FILE DOWNLOADED FROM DRIVE AFTER TRAINING ON COLAB.\n",
    "# ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "#                            decoder=decoder,\n",
    "#                            optimizer = optimizer)\n",
    "# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12004,
     "status": "ok",
     "timestamp": 1573126487039,
     "user": {
      "displayName": "Chetan Borse",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAGfeT8L1pcvT3RQVudl-BtKxdIj2DmV-CeqEkVXQ=s64",
      "userId": "11710927843617506938"
     },
     "user_tz": -330
    },
    "id": "k35ll7XXYl8Q",
    "outputId": "82992e5d-2f32-463c-c265-990564de91d5"
   },
   "outputs": [],
   "source": [
    "# ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1FSHEN31VPVu"
   },
   "outputs": [],
   "source": [
    "# def evaluate(image):\n",
    "#     attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "#     hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "#     # temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "#     # img_tensor_val = image_features_extract_model(image)\n",
    "#     # img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "#     features = encoder(image)\n",
    "\n",
    "#     dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "#     result = []\n",
    "\n",
    "#     for i in range(max_length):\n",
    "#         predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "# #         attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "#         predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "#         result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "#         if tokenizer.index_word[predicted_id] == '<end>':\n",
    "#             return result, attention_plot\n",
    "\n",
    "#         dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "#     attention_plot = attention_plot[:len(result), :]\n",
    "#     return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(frame):\n",
    "#     img = np.expand_dims(frame, axis=0)\n",
    "#     img = tf.image.resize(img, (299, 299))\n",
    "#     img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "#     feature = image_features_extract_model(img)\n",
    "#     feature = tf.reshape(feature, (feature.shape[0], -1, feature.shape[3]))\n",
    "#     return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread('/home/avitra/rakathon/uploaded/testimg1.jpg')\n",
    "# # cv2.imshow('frame', image)\n",
    "# # cv2.waitKey(0)\n",
    "# # cv2.destroyAllWindows()\n",
    "# name = recognize.recognize(image)\n",
    "# feature = preprocess(image)\n",
    "# result, _ = evaluate(feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(2)\n",
    "# frame_count = 0\n",
    "# # placeholders = [' a man', ' a person', ' a boy']\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if cv2.waitKey(1)==27:\n",
    "#         break\n",
    "#     if not ret:\n",
    "#         pass\n",
    "#     else:\n",
    "#         frame_count+=1\n",
    "#         cv2.imshow('frame', frame)\n",
    "#         if frame_count%250==0:\n",
    "#             name = recognize.recognize(frame)\n",
    "#             feature = preprocess(frame)\n",
    "#             result, _ = evaluate(feature)\n",
    "#             text = ''\n",
    "#             for i in result:\n",
    "#                 text += i+' '\n",
    "#             text = text[:text.find('<end>')]\n",
    "#             print(text)\n",
    "# #             if len(name)>0:\n",
    "# #                 for i in placeholders:\n",
    "# #                     if i not in text:\n",
    "# #                         pass\n",
    "# #                     else:\n",
    "# #                         name_index = text.find(i)+1\n",
    "# #                         text = text[:name_index]+'{}'.format(name[0])+text[name_index+len(i)-1:]\n",
    "# #             print(text)\n",
    "#             myobj = gTTS(text=text, lang=language, slow=False)\n",
    "#             myobj.save(\"audio.mp3\")\n",
    "#             playsound('audio.mp3')\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "restore_model",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
